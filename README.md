# Microservice Ollama - Python API

## ğŸŒ Description

### [ğŸ‡¬ğŸ‡§ English]
This project is a Python microservice providing an API with three endpoints. Each endpoint interacts with a different Ollama model to generate responses based on user input.

### [ğŸ‡«ğŸ‡· FranÃ§ais]
Ce projet est un microservice Python fournissant une API avec trois endpoints. Chaque endpoint interagit avec un modÃ¨le Ollama diffÃ©rent pour gÃ©nÃ©rer des rÃ©ponses basÃ©es sur l'entrÃ©e de l'utilisateur.

### [ğŸ‡ªğŸ‡¸ EspaÃ±ol]
Este proyecto es un microservicio en Python que proporciona una API con tres endpoints. Cada endpoint interactÃºa con un modelo Ollama diferente para generar respuestas basadas en la entrada del usuario.

### [ğŸ‡µğŸ‡¹ PortuguÃªs]
Este projeto Ã© um microsserviÃ§o em Python que fornece uma API com trÃªs endpoints. Cada endpoint interage com um modelo Ollama diferente para gerar respostas com base na entrada do usuÃ¡rio.

### [ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª]
ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€Pythonãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã§ã€3ã¤ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æŒã¤APIã‚’æä¾›ã—ã¾ã™ã€‚å„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯ç•°ãªã‚‹Ollamaãƒ¢ãƒ‡ãƒ«ã¨å¯¾è©±ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«åŸºã¥ã„ã¦å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

### [ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ¸Ğ¹]
Ğ­Ñ‚Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ ÑĞ¾Ğ±Ğ¾Ğ¹ Ğ¼Ğ¸ĞºÑ€Ğ¾ÑĞµÑ€Ğ²Ğ¸Ñ Ğ½Ğ° Python, Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‰Ğ¸Ğ¹ API Ñ Ñ‚Ñ€ĞµĞ¼Ñ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¼Ğ¸ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸. ĞšĞ°Ğ¶Ğ´Ğ°Ñ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ²Ğ·Ğ°Ğ¸Ğ¼Ğ¾Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ Ollama Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ.

### [ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©]
Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ù‡Ùˆ Ø®Ø¯Ù…Ø© Ù…ØµØºØ±Ø© Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ù„ØºØ© Ø¨Ø§ÙŠØ«ÙˆÙ† ØªÙˆÙØ± ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø¨Ø«Ù„Ø§Ø« Ù†Ù‚Ø§Ø· Ù†Ù‡Ø§ÙŠØ©. ÙŠØªÙØ§Ø¹Ù„ ÙƒÙ„ Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø®ØªÙ„Ù Ù…Ù† Ollama Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….

---

## ğŸš€ Endpoints

### `/api/general`
- Uses **Llama3.2** model to generate a response to general queries.
   ```sh
   ollama run llama3.2:3b
   ```

### `/api/economic`
- Uses **Deepseek-r1:8b** model to provide economic insights.
   ```sh
   ollama run deepseek-r1:8b
   ```
### `/api/legal`
- Uses **Gemma** model to generate legal-related responses.
   ```sh
   ollama run gemma:7b
   ```
---

## ğŸ›  Installation & Execution

### [ğŸ‡¬ğŸ‡§ English]
1. Clone the repository:
   ```sh
   git clone https://github.com/your-repo/ollama-microservice.git
   cd ollama-microservice
   ```
2. Install dependencies:
   ```sh
   pip install -r requirements.txt
   ```
3. Start the service:
   ```sh
   uvicorn app.main:app --host 0.0.0.0 --port 8000
   ```

### [ğŸ‡«ğŸ‡· FranÃ§ais]
1. Cloner le dÃ©pÃ´t :
   ```sh
   git clone https://github.com/your-repo/ollama-microservice.git
   cd ollama-microservice
   ```
2. Installer les dÃ©pendances :
   ```sh
   pip install -r requirements.txt
   ```
3. DÃ©marrer le service :
   ```sh
   uvicorn app.main:app --host 0.0.0.0 --port 8000
   ```

### [ğŸ‡ªğŸ‡¸ EspaÃ±ol]
1. Clonar el repositorio:
   ```sh
   git clone https://github.com/your-repo/ollama-microservice.git
   cd ollama-microservice
   ```
2. Instalar dependencias:
   ```sh
   pip install -r requirements.txt
   ```
3. Iniciar el servicio:
   ```sh
   uvicorn app.main:app --host 0.0.0.0 --port 8000
   ```

---

## ğŸ“„ License
This project is licensed under the MIT License.

