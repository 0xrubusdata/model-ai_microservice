import json
import requests

def ollama_generate(model: str, prompt: str) -> dict:
    """
    Appelle l'API HTTP d'Ollama pour exÃ©cuter un modÃ¨le donnÃ© avec un prompt.
    
    # [ğŸ‡¬ğŸ‡§ Calls the Ollama HTTP API to execute a given model with a prompt.] 
    # [ğŸ‡ªğŸ‡¸ Llama a la API HTTP de Ollama para ejecutar un modelo con un prompt.] 
    # [ğŸ‡µğŸ‡¹ Chama a API HTTP do Ollama para executar um modelo com um prompt.] 
    # [ğŸ‡¯ğŸ‡µ Ollama ã® HTTP API ã‚’å‘¼ã³å‡ºã—ã¦ã€æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å®Ÿè¡Œã—ã¾ã™ã€‚] 
    # [ğŸ‡·ğŸ‡º Ğ’Ñ‹Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚ HTTP API Ollama Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ¼.] 
    # [ğŸ‡¸ğŸ‡¦ ÙŠØ³ØªØ¯Ø¹ÙŠ API HTTP Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ Ollama Ù„ØªÙ†ÙÙŠØ° Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆØ¬Ù‡.]

    Args:
        model (str): Le nom du modÃ¨le Ã  exÃ©cuter.
        # [ğŸ‡¬ğŸ‡§ The name of the model to execute.] | [ğŸ‡ªğŸ‡¸ El nombre del modelo a ejecutar.] | [ğŸ‡µğŸ‡¹ O nome do modelo a executar.] | [ğŸ‡¯ğŸ‡µ å®Ÿè¡Œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®åå‰ã€‚] | [ğŸ‡·ğŸ‡º Ğ˜Ğ¼Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ.] | [ğŸ‡¸ğŸ‡¦ Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ´ØºÙŠÙ„Ù‡.]
        
        prompt (str): La question ou le prompt Ã  envoyer au modÃ¨le.
        # [ğŸ‡¬ğŸ‡§ The question or prompt to send to the model.] | [ğŸ‡ªğŸ‡¸ La pregunta o prompt a enviar al modelo.] | [ğŸ‡µğŸ‡¹ A pergunta ou prompt a enviar para o modelo.] | [ğŸ‡¯ğŸ‡µ ãƒ¢ãƒ‡ãƒ«ã«é€ä¿¡ã™ã‚‹è³ªå•ã¾ãŸã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‚] | [ğŸ‡·ğŸ‡º Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¸Ğ»Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.] | [ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ù„Ø¥Ø±Ø³Ø§Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.]

    Returns:
        dict: La rÃ©ponse JSON complÃ¨te de l'API Ollama.
        # [ğŸ‡¬ğŸ‡§ The complete JSON response from the Ollama API.] | [ğŸ‡ªğŸ‡¸ La respuesta JSON completa de la API de Ollama.] | [ğŸ‡µğŸ‡¹ A resposta JSON completa da API Ollama.] | [ğŸ‡¯ğŸ‡µ Ollama API ã‹ã‚‰ã®å®Œå…¨ãª JSON å¿œç­”ã€‚] | [ğŸ‡·ğŸ‡º ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ JSON-Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ API Ollama.] | [ğŸ‡¸ğŸ‡¦ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON Ù…Ù† API Ollama.]
    """
    url = "http://localhost:11434/api/generate"

    # Format attendu par les modÃ¨les
    # [ğŸ‡¬ğŸ‡§ Expected format for models] | [ğŸ‡ªğŸ‡¸ Formato esperado por los modelos] | [ğŸ‡µğŸ‡¹ Formato esperado pelos modelos] | [ğŸ‡¯ğŸ‡µ ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ] | [ğŸ‡·ğŸ‡º ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹] | [ğŸ‡¸ğŸ‡¦ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬]
    expected_format = {
        "type": "object",
        "properties": {
            "question": {
                "type": "string"
            },
            "answer": {
                "type": "string"
            }
        },
        "required": ["question", "answer"]
    }

    # Payload de la requÃªte
    # [ğŸ‡¬ğŸ‡§ Request payload] | [ğŸ‡ªğŸ‡¸ Carga Ãºtil de la solicitud] | [ğŸ‡µğŸ‡¹ Payload da requisiÃ§Ã£o] | [ğŸ‡¯ğŸ‡µ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰] | [ğŸ‡·ğŸ‡º ĞŸĞ¾Ğ»ĞµĞ·Ğ½Ğ°Ñ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°] | [ğŸ‡¸ğŸ‡¦ Ø­Ù…ÙˆÙ„Ø© Ø§Ù„Ø·Ù„Ø¨]
    payload = {
        "model": model,
        "prompt": prompt,
        "format": expected_format,
        "stream": False
    }

    try:
        # Envoi de la requÃªte
        # [ğŸ‡¬ğŸ‡§ Sending the request] | [ğŸ‡ªğŸ‡¸ Enviando la solicitud] | [ğŸ‡µğŸ‡¹ Enviando a requisiÃ§Ã£o] | [ğŸ‡¯ğŸ‡µ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®é€ä¿¡] | [ğŸ‡·ğŸ‡º ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°] | [ğŸ‡¸ğŸ‡¦ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨]
        response = requests.post(url, json=payload)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        # Gestion des erreurs HTTP
        # [ğŸ‡¬ğŸ‡§ Handling HTTP errors] | [ğŸ‡ªğŸ‡¸ Manejo de errores HTTP] | [ğŸ‡µğŸ‡¹ Tratamento de erros HTTP] | [ğŸ‡¯ğŸ‡µ HTTP ã‚¨ãƒ©ãƒ¼ã®å‡¦ç†] | [ğŸ‡·ğŸ‡º ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº HTTP] | [ğŸ‡¸ğŸ‡¦ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ HTTP]
        return {"error": str(e), "response": None}
