#!/bin/bash

# Attendre qu'Ollama soit prÃªt
# [ğŸ‡¬ğŸ‡§ Waiting for Ollama to be ready] | [ğŸ‡ªğŸ‡¸ Esperando que Ollama estÃ© listo] | [ğŸ‡µğŸ‡¹ Aguardando Ollama ficar pronto] | [ğŸ‡¯ğŸ‡µ Ollama ã®æº–å‚™ãŒã§ãã‚‹ã®ã‚’å¾…ã£ã¦ã„ã¾ã™] | [ğŸ‡·ğŸ‡º ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸ Ollama] | [ğŸ‡¸ğŸ‡¦ Ø§Ù†ØªØ¸Ø§Ø± Ø¬Ø§Ù‡Ø²ÙŠØ© Ollama]
echo "Waiting for Ollama to be available..."
until curl -s http://localhost:11434/api/models > /dev/null; do
  # Ollama n'est pas encore prÃªt. Nouvelle tentative dans 5 secondes...
  # [ğŸ‡¬ğŸ‡§ Ollama is not ready yet. Retrying in 5 seconds...] | [ğŸ‡ªğŸ‡¸ Ollama aÃºn no estÃ¡ listo. Reintentando en 5 segundos...] | [ğŸ‡µğŸ‡¹ Ollama ainda nÃ£o estÃ¡ pronto. Tentando novamente em 5 segundos...] | [ğŸ‡¯ğŸ‡µ Ollama ã¯ã¾ã æº–å‚™ãŒã§ãã¦ã„ã¾ã›ã‚“ã€‚5ç§’å¾Œã«å†è©¦è¡Œ...] | [ğŸ‡·ğŸ‡º Ollama ĞµÑ‰Ğµ Ğ½Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ². ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· 5 ÑĞµĞºÑƒĞ½Ğ´...] | [ğŸ‡¸ğŸ‡¦ Ollama Ù„Ù… ÙŠØµØ¨Ø­ Ø¬Ø§Ù‡Ø²Ù‹Ø§ Ø¨Ø¹Ø¯. Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ ÙÙŠ 5 Ø«ÙˆØ§Ù†Ù...]
  echo "Ollama is not ready yet. Retrying in 5 seconds..."
  sleep 5
done

# TÃ©lÃ©charger les modÃ¨les nÃ©cessaires
# [ğŸ‡¬ğŸ‡§ Downloading necessary models] | [ğŸ‡ªğŸ‡¸ Descargando modelos necesarios] | [ğŸ‡µğŸ‡¹ Baixando modelos necessÃ¡rios] | [ğŸ‡¯ğŸ‡µ å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­] | [ğŸ‡·ğŸ‡º Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹] | [ğŸ‡¸ğŸ‡¦ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©]
echo "Downloading models with Ollama..."
ollama pull gemma
ollama pull deepseek-r1:8b
ollama pull llama3.2

# Lancer l'application FastAPI
# [ğŸ‡¬ğŸ‡§ Starting FastAPI application] | [ğŸ‡ªğŸ‡¸ Iniciando aplicaciÃ³n FastAPI] | [ğŸ‡µğŸ‡¹ Iniciando aplicaÃ§Ã£o FastAPI] | [ğŸ‡¯ğŸ‡µ FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•] | [ğŸ‡·ğŸ‡º Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ FastAPI] | [ğŸ‡¸ğŸ‡¦ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ ØªØ·Ø¨ÙŠÙ‚ FastAPI]
echo "Starting FastAPI application..."
uvicorn app.main:app --host 0.0.0.0 --port 8000
